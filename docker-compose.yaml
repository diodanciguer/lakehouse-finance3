# docker-compose.yml — Airflow 3 stack (sem 'version:')

x-airflow-image: &airflow_image apache/airflow:3.0.6
x-airflow-env: &airflow_env
  AIRFLOW_UID: ${AIRFLOW_UID}
  AIRFLOW_GID: ${AIRFLOW_GID}
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
  AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
  AIRFLOW__CORE__AUTH_MANAGER: ${AIRFLOW__CORE__AUTH_MANAGER}
  SQL_ALCHEMY_CONN: ${SQL_ALCHEMY_CONN}
  CELERY_BROKER_URL: ${CELERY_BROKER_URL}
  CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
  _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS}

services:
  postgres3:
    image: postgres:16
    container_name: code-postgres3-1
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - ./pgdata:/var/lib/postgresql/data

  redis3:
    image: redis:7-alpine
    container_name: code-redis3-1
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow3-init:
    image: *airflow_image
    container_name: code-airflow3-init-1
    depends_on:
      postgres3:
        condition: service_healthy
      redis3:
        condition: service_healthy
    environment:
      <<: *airflow_env
      # credenciais do admin p/ criação de usuário
      AIRFLOW_ADMIN_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    entrypoint: ["/bin/bash","-c"]
    command: >
      "
      set -euo pipefail
      echo '==> Migrando DB'
      airflow db migrate
      echo '==> Criando/atualizando usuário admin'
      airflow users create
        --username \"${AIRFLOW_ADMIN_USERNAME}\"
        --password \"${AIRFLOW_ADMIN_PASSWORD}\"
        --firstname Diego
        --lastname Admin
        --role Admin
        --email \"${AIRFLOW_ADMIN_EMAIL}\" || true
      echo '==> Sincronizando permissões'
      airflow sync-perm || true
      echo '==> Init concluído'
      "

  airflow3-scheduler:
    image: *airflow_image
    container_name: code-airflow3-scheduler-1
    depends_on:
      postgres3:
        condition: service_healthy
      redis3:
        condition: service_healthy
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","scheduler"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    restart: unless-stopped

  airflow3-triggerer:
    image: *airflow_image
    container_name: code-airflow3-triggerer-1
    depends_on:
      postgres3:
        condition: service_healthy
      redis3:
        condition: service_healthy
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","triggerer"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    restart: unless-stopped

  airflow3-dag-processor:
    image: *airflow_image
    container_name: code-airflow3-dag-processor-1
    depends_on:
      postgres3:
        condition: service_healthy
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","dag-processor"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    restart: unless-stopped

  airflow3-worker:
    image: *airflow_image
    container_name: code-airflow3-worker-1
    depends_on:
      postgres3:
        condition: service_healthy
      redis3:
        condition: service_healthy
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","celery","worker"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    restart: unless-stopped

  airflow3-webserver:
    # Mantém o nome do serviço que você já tem, mas roda o novo comando 'api-server'
    image: *airflow_image
    container_name: code-airflow3-webserver-1
    depends_on:
      postgres3:
        condition: service_healthy
      redis3:
        condition: service_healthy
      airflow3-scheduler:
        condition: service_started
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","api-server","--port","8080"]
    # mapeia para 8083 no host para NÃO colidir com seu Airflow 2.x
    ports:
      - "8083:8080"
    healthcheck:
      test: ["CMD-SHELL","curl -sf http://localhost:8080/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    restart: unless-stopped

  flower3:
    image: *airflow_image
    container_name: code-flower3-1
    depends_on:
      redis3:
        condition: service_healthy
    environment: *airflow_env
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: ["airflow","celery","flower","--port","5555"]
    # mapeia para 5563 no host para não colidir com outro projeto
    ports:
      - "5563:5555"
    restart: unless-stopped

volumes:
  # apenas para clareza; estamos usando bind mounts ./dags, ./logs, ./plugins
  # e um diretório local ./pgdata para o Postgres
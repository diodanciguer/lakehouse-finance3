# docker compose v2+ (sem "version")
# Airflow 3.0.6 com CeleryExecutor e API Server
# Sufixo "3" para não colidir com seu Airflow 2.x

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.6}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres3:5432/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres3:5432/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis3:6379/0

    # Webserver por trás de proxy + domínio público
    AIRFLOW__WEBSERVER__BASE_URL: ${AIRFLOW__WEBSERVER__BASE_URL:-https://lakehouse-finance3-airflow3.hjbbqx.easypanel.host}
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: ${AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX:-True}
    AIRFLOW__WEBSERVER__COOKIE_SECURE: ${AIRFLOW__WEBSERVER__COOKIE_SECURE:-True}
    AIRFLOW__WEBSERVER__COOKIE_SAMESITE: ${AIRFLOW__WEBSERVER__COOKIE_SAMESITE:-None}
    
    # Configurações críticas do proxy reverso
    AIRFLOW__WEBSERVER__PROXY_FIX_X_FOR: ${AIRFLOW__WEBSERVER__PROXY_FIX_X_FOR:-1}
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PROTO: ${AIRFLOW__WEBSERVER__PROXY_FIX_X_PROTO:-1}
    AIRFLOW__WEBSERVER__PROXY_FIX_X_HOST: ${AIRFLOW__WEBSERVER__PROXY_FIX_X_HOST:-1}
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PORT: ${AIRFLOW__WEBSERVER__PROXY_FIX_X_PORT:-1}
    AIRFLOW__WEBSERVER__PROXY_FIX_X_PREFIX: ${AIRFLOW__WEBSERVER__PROXY_FIX_X_PREFIX:-1}
    
    # Restringe hosts aceitos pela UI
    AIRFLOW__WEBSERVER__ALLOWED_HOSTS: ${AIRFLOW__WEBSERVER__ALLOWED_HOSTS:-lakehouse-finance3-airflow3.hjbbqx.easypanel.host}

    # Secrets (API e sessão)
    AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY:-change-this-to-a-long-random-string}
    AIRFLOW_SECRET_KEY: ${AIRFLOW_SECRET_KEY:-change-this-to-a-long-random-string}

    # Fuso horário
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ${TZ:-America/Sao_Paulo}
    TZ: ${TZ:-America/Sao_Paulo}

    # (opcional mas recomendado) criptografa senhas de Connections
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-}

    # Auth manager FAB (UI de login)
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

    # Provider FAB para UI/API baseada em sessão
    AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS:-airflow.providers.fab.auth_manager.api.auth.backend.session.SessionAuthBackend}

    # Logging local (Airflow 3 exige 'task')
    AIRFLOW__LOGGING__TASK_LOG_READER: task
    AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW__LOGGING__REMOTE_LOGGING:-False}
    AIRFLOW__LOGGING__BASE_LOG_FOLDER: ${AIRFLOW__LOGGING__BASE_LOG_FOLDER:-/opt/airflow/logs}
    AIRFLOW__LOGGING__DAG_PROCESSOR_LOG_TARGET: ${AIRFLOW__LOGGING__DAG_PROCESSOR_LOG_TARGET:-/opt/airflow/logs/dag_processor}
    AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL:-INFO}

    # Instala provider FAB na subida (ok para dev)
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-apache-airflow-providers-fab}

    # Evita proxy e força tráfego direto entre containers
    NO_PROXY: ${NO_PROXY:-localhost,127.0.0.1,airflow3-api-server,postgres3,redis3}
  user: "${AIRFLOW_UID:-50000}:0"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
    - ./entrypoint.sh:/opt/airflow/entrypoint.sh
  depends_on:
    postgres3:
      condition: service_healthy
    redis3:
      condition: service_healthy

services:
  postgres3:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres3-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 20
    restart: unless-stopped

  redis3:
    image: redis:7-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 20
    restart: unless-stopped

  # Roda uma vez: migra DB e cria admin se não existir (usa _AIRFLOW_WWW_USER_* do Easypanel)
  airflow3-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -e
        echo ">>> Migrando DB..."
        airflow db migrate

        if [ -z "$_AIRFLOW_WWW_USER_USERNAME" ] || [ -z "$_AIRFLOW_WWW_USER_PASSWORD" ] || [ -z "$_AIRFLOW_WWW_USER_EMAIL" ]; then
          echo "ERRO: variáveis _AIRFLOW_WWW_USER_* não definidas. Abortando criação do usuário."
          exit 1
        fi

        echo ">>> Verificando se o usuário ${_AIRFLOW_WWW_USER_USERNAME} existe..."
        if ! airflow users list --output yaml | grep -q " username: ${_AIRFLOW_WWW_USER_USERNAME}$"; then
          echo ">>> Criando usuário admin ${_AIRFLOW_WWW_USER_USERNAME}..."
          airflow users create \
            --role Admin \
            --username "$_AIRFLOW_WWW_USER_USERNAME" \
            --password "$_AIRFLOW_WWW_USER_PASSWORD" \
            --email "$_AIRFLOW_WWW_USER_EMAIL" \
            --firstname Admin --lastname User
        else
          echo ">>> Usuário já existe. Pulando criação."
        fi
    restart: "no"

  airflow3-api-server:
    <<: *airflow-common
    # Configurações específicas do API server
    environment:
      <<: *airflow-common-env
      # Força o API server a usar a URL pública - SEM fallback
      AIRFLOW__WEBSERVER__BASE_URL: https://lakehouse-finance3-airflow3.hjbbqx.easypanel.host
      # Força SSL/HTTPS
      AIRFLOW__WEBSERVER__FORCE_SECURE: "True"
      # Desabilita redirects internos
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
      # Força host e esquema corretos
      FLASK_ENV: production
      PREFERRED_URL_SCHEME: https
      SERVER_NAME: lakehouse-finance3-airflow3.hjbbqx.easypanel.host
      # Força conteúdo a ser servido via HTTPS
      AIRFLOW__WEBSERVER__X_FRAME_OPTIONS: SAMEORIGIN
    # Comando simplificado para debug - aplicar patches e iniciar api-server
    command: 
      - bash
      - -c
      - |
        set -e
        echo "[DEBUG] Iniciando Airflow API Server..."
        echo "[DEBUG] PYTHONPATH: $$PYTHONPATH"
        echo "[DEBUG] BASE_URL: $$AIRFLOW__WEBSERVER__BASE_URL"
        
        # Aplicar patches
        export PYTHONPATH="/opt/airflow/config:$$PYTHONPATH"
        python3 /opt/airflow/config/init_patches.py
        
        echo "[DEBUG] Iniciando API server na porta 8080..."
        exec airflow api-server --port 8080 --hostname 0.0.0.0
    # Não expor portas no Easypanel - o proxy interno cuida disso
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health >/dev/null"]
      interval: 10s
      timeout: 10s
      retries: 30
    restart: unless-stopped

  airflow3-scheduler:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      # Scheduler usa o webserver para API
      AIRFLOW__API__BASE_URL: http://airflow3-api-server:8080
    command: ["bash","-c","exec airflow scheduler"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-triggerer:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      # Triggerer usa o webserver para API
      AIRFLOW__API__BASE_URL: http://airflow3-api-server:8080
    command: ["bash","-c","exec airflow triggerer"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-dag-processor:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      # DAG processor usa o webserver para API
      AIRFLOW__API__BASE_URL: http://airflow3-api-server:8080
    command: ["bash","-c","exec airflow dag-processor"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-worker:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      # Worker usa o webserver para API
      AIRFLOW__API__BASE_URL: http://airflow3-api-server:8080
    command: ["bash","-c","exec airflow celery worker"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  flower3:
    <<: *airflow-common
    # Flower não precisa do API__BASE_URL
    command: ["bash","-c","exec airflow celery flower"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

volumes:
  postgres3-data:
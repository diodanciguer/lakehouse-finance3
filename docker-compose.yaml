# docker compose v2+ (sem "version")
# Airflow 3.0.6 com CeleryExecutor e API Server
# Sufixo "3" para não colidir com seu Airflow 2.x

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.6}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres3:5432/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres3:5432/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis3:6379/0

    # Webserver por trás de proxy + domínio público
    AIRFLOW__WEBSERVER__BASE_URL: https://lakehouse-finance3-airflow3.hjbbqx.easypanel.host
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
    AIRFLOW__WEBSERVER__COOKIE_SECURE: "True"
    AIRFLOW__WEBSERVER__COOKIE_SAMESITE: Lax

    # No Airflow 3 o secret fica em [api]
    AIRFLOW__API__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-change-this-to-a-long-random-string}

    # Fuso horário
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ${TZ:-America/Sao_Paulo}

    # (opcional mas recomendado) criptografa senhas de Connections
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-}

    # Auth manager FAB (UI de login)
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

    # Instala provider FAB na subida (ok para dev)
    _PIP_ADDITIONAL_REQUIREMENTS: apache-airflow-providers-fab
  user: "${AIRFLOW_UID:-50000}:0"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
  depends_on:
    postgres3:
      condition: service_healthy
    redis3:
      condition: service_healthy

services:
  postgres3:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres3-data:/var/lib/postgresql/data   # volume nomeado (persiste)
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 20
    restart: unless-stopped

  redis3:
    image: redis:7-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 20
    restart: unless-stopped

  # Roda uma vez: migra DB e cria admin se não existir (usa _AIRFLOW_WWW_USER_* do Easypanel)
  airflow3-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -e
        echo ">>> Migrando DB..."
        airflow db migrate

        if [ -z "$_AIRFLOW_WWW_USER_USERNAME" ] || [ -z "$_AIRFLOW_WWW_USER_PASSWORD" ] || [ -z "$_AIRFLOW_WWW_USER_EMAIL" ]; then
          echo "ERRO: variáveis _AIRFLOW_WWW_USER_* não definidas. Abortando criação do usuário."
          exit 1
        fi

        echo ">>> Verificando se o usuário ${_AIRFLOW_WWW_USER_USERNAME} existe..."
        if ! airflow users list --output yaml | grep -q " username: ${_AIRFLOW_WWW_USER_USERNAME}$"; then
          echo ">>> Criando usuário admin ${_AIRFLOW_WWW_USER_USERNAME}..."
          airflow users create \
            --role Admin \
            --username "$_AIRFLOW_WWW_USER_USERNAME" \
            --password "$_AIRFLOW_WWW_USER_PASSWORD" \
            --email "$_AIRFLOW_WWW_USER_EMAIL" \
            --firstname Admin --lastname User
        else
          echo ">>> Usuário já existe. Pulando criação."
        fi
    restart: "no"

  airflow3-api-server:
    <<: *airflow-common
    command: ["bash","-c","exec airflow api-server"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    # healthcheck: valida o webserver HTTP (endpoint público de login)
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health | grep -q '\"status\": \"healthy\"'"]
      interval: 10s
      timeout: 10s
      retries: 30
    restart: unless-stopped

  airflow3-scheduler:
    <<: *airflow-common
    command: ["bash","-c","exec airflow scheduler"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-triggerer:
    <<: *airflow-common
    command: ["bash","-c","exec airflow triggerer"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-dag-processor:
    <<: *airflow-common
    command: ["bash","-c","exec airflow dag-processor"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  airflow3-worker:
    <<: *airflow-common
    command: ["bash","-c","exec airflow celery worker"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

  flower3:
    <<: *airflow-common
    command: ["bash","-c","exec airflow celery flower"]
    depends_on:
      airflow3-init:
        condition: service_completed_successfully
    restart: unless-stopped

volumes:
  postgres3-data: